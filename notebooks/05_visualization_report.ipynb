{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 05 - Visualization & Report Generation\n",
    "Generate presentation-ready visualizations and the final Fraudasaurus report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from src.visualize import (\n",
    "    plot_amount_histogram, plot_volume_by_time, plot_risk_heatmap,\n",
    "    plot_network_graph, plot_account_timeline, plot_anomaly_scatter\n",
    ")\n",
    "from src.report import generate_report, generate_markdown_report\n",
    "from src.features import build_feature_matrix\n",
    "\n",
    "figures_dir = Path(\"../output/figures\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/processed\")\n",
    "datasets = {f.stem: pd.read_parquet(f) for f in data_dir.glob(\"*.parquet\")}\n",
    "output_dir = Path(\"../output\")\n",
    "composite = pd.read_csv(output_dir / \"risk_scores.csv\") if (output_dir / \"risk_scores.csv\").exists() else pd.DataFrame()\n",
    "\n",
    "txn_key = next((k for k in datasets if \"trans\" in k.lower()), list(datasets.keys())[0] if datasets else None)\n",
    "txn = datasets[txn_key] if txn_key else pd.DataFrame()\n",
    "print(f\"Transaction data: {txn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 1. Transaction Amount Distribution (Structuring Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(txn) > 0:\n",
    "    amt_col = next((c for c in txn.columns if \"amount\" in c.lower()), None)\n",
    "    if amt_col:\n",
    "        fig = plot_amount_histogram(txn, amount_col=amt_col, save_path=str(figures_dir / \"amount_histogram.png\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 2. Transaction Volume Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(txn) > 0:\n",
    "    date_col = next((c for c in txn.columns if \"date\" in c.lower()), None)\n",
    "    if date_col:\n",
    "        fig = plot_volume_by_time(txn, date_col=date_col, save_path=str(figures_dir / \"volume_by_time.png\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 3. Risk Score Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(composite) > 0:\n",
    "    # Build heatmap data from individual detector scores\n",
    "    detector_files = list(output_dir.glob(\"detector_*.csv\"))\n",
    "    heatmap_data = {}\n",
    "    for f in detector_files:\n",
    "        name = f.stem.replace(\"detector_\", \"\")\n",
    "        det_df = pd.read_csv(f)\n",
    "        if \"account_id\" in det_df.columns and \"risk_score\" in det_df.columns:\n",
    "            heatmap_data[name] = det_df.set_index(\"account_id\")[\"risk_score\"]\n",
    "    \n",
    "    if heatmap_data:\n",
    "        heatmap_df = pd.DataFrame(heatmap_data).fillna(0)\n",
    "        # Show top 30 riskiest accounts\n",
    "        top_accounts = composite.head(30)[\"account_id\"].tolist()\n",
    "        heatmap_subset = heatmap_df.loc[heatmap_df.index.isin(top_accounts)]\n",
    "        if len(heatmap_subset) > 0:\n",
    "            fig = plot_risk_heatmap(heatmap_subset, save_path=str(figures_dir / \"risk_heatmap.png\"))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection Scatter (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(txn) > 0:\n",
    "    try:\n",
    "        features = build_feature_matrix(txn)\n",
    "        # Load anomaly results if available\n",
    "        anomaly_path = output_dir / \"detector_anomaly.csv\"\n",
    "        labels = None\n",
    "        if anomaly_path.exists():\n",
    "            anomaly_df = pd.read_csv(anomaly_path)\n",
    "            if \"account_id\" in anomaly_df.columns and \"risk_score\" in anomaly_df.columns:\n",
    "                merged = features.merge(anomaly_df[[\"account_id\", \"risk_score\"]], on=\"account_id\", how=\"left\")\n",
    "                labels = np.where(merged[\"risk_score\"] > 0.5, -1, 1)\n",
    "                features = merged.drop(columns=[\"risk_score\"])\n",
    "        fig = plot_anomaly_scatter(features.select_dtypes(include=[np.number]), labels=labels,\n",
    "                                  save_path=str(figures_dir / \"anomaly_scatter.png\"))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not build feature matrix: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "## 5. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_context = {\n",
    "    \"title\": \"Fraudasaurus.ai - ARFI Fraud Detection Report\",\n",
    "    \"generated_at\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "    \"executive_summary\": (\n",
    "        \"Fraudasaurus.ai analyzed ARFI's financial transaction data using five complementary \"\n",
    "        \"fraud detection methods: structuring detection, account takeover detection, check kiting \"\n",
    "        \"analysis, dormant account monitoring, and unsupervised anomaly detection. \"\n",
    "        \"Accounts were scored across all detectors and ranked by composite risk.\"\n",
    "    ),\n",
    "    \"data_overview\": {\n",
    "        \"Datasets loaded\": len(datasets),\n",
    "        \"Total transactions\": len(txn) if len(txn) > 0 else \"N/A\",\n",
    "        \"Risk scores computed\": len(composite) if len(composite) > 0 else \"N/A\",\n",
    "        \"CRITICAL accounts\": len(composite[composite[\"risk_tier\"] == \"CRITICAL\"]) if \"risk_tier\" in composite.columns else \"N/A\",\n",
    "        \"HIGH accounts\": len(composite[composite[\"risk_tier\"] == \"HIGH\"]) if \"risk_tier\" in composite.columns else \"N/A\",\n",
    "    },\n",
    "    \"findings\": [\n",
    "        {\"name\": \"Structuring (BSA/CTR Evasion)\", \"description\": \"Transactions clustered below $10K reporting threshold.\", \"figure_path\": \"figures/amount_histogram.png\"},\n",
    "        {\"name\": \"Risk Score Heatmap\", \"description\": \"Multi-detector risk view of highest-risk accounts.\", \"figure_path\": \"figures/risk_heatmap.png\"},\n",
    "        {\"name\": \"Anomaly Detection\", \"description\": \"Isolation Forest identified statistical outliers.\", \"figure_path\": \"figures/anomaly_scatter.png\"},\n",
    "    ],\n",
    "    \"carmeg_profile\": \"See notebook 04_carmeg_hunt.ipynb for full CarMeg SanDiego analysis.\",\n",
    "    \"proposed_solution\": (\n",
    "        \"A real-time fraud scoring pipeline for ARFI:\\n\"\n",
    "        \"1. Data Layer: Consume existing Jack Henry core + Banno digital feeds\\n\"\n",
    "        \"2. Detection Layer: 4 specialized rule-based detectors + 1 ML anomaly detector\\n\"\n",
    "        \"3. Scoring Layer: Weighted combination with configurable risk tiers\\n\"\n",
    "        \"4. Alert Layer: CRITICAL=immediate, HIGH=daily review, MEDIUM=monthly trending\\n\"\n",
    "        \"5. Dashboard: Risk leaderboard with drill-down explainability\\n\\n\"\n",
    "        \"Feasibility: Uses existing data, auditable rules, local ML (no data leaves ARFI), \"\n",
    "        \"can start as nightly batch and evolve to near-real-time.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "html = generate_report(report_context, output_path=\"../output/report.html\")\n",
    "md = generate_markdown_report(report_context, output_path=\"../output/report.md\")\n",
    "print(\"Reports generated:\")\n",
    "print(f\"  HTML: output/report.html ({len(html)} chars)\")\n",
    "print(f\"  Markdown: output/report.md ({len(md)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## Done!\n",
    "Reports and figures saved to `output/`. Ready for presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}